import argparse
import os
import torch
from torch.utils.data import DataLoader
from datasets import get_dataset
from model_manager import SAMModelManager

# Importaciones opcionales que se cargan dinÃ¡micamente segÃºn necesidad
try:
    from transformers import SamModel, SamProcessor
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âš ï¸  Transformers no disponible, usando modelos PyTorch nativos")

try:
    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False
    print("âš ï¸  PEFT no disponible, usando congelamiento manual")

# ConfiguraciÃ³n basada en lo disponible en SAMModelManager
MODEL_CONFIG = {
    "sam2": {
        "manager_family": "sam2",                # SAM2 - Next generation
        "manager_variant": "hiera_tiny",         # Variante tiny mÃ¡s liviana
        "description": "SAM2 - Next generation Segment Anything Model"
    },
    "medsam2": {
        "manager_family": "medsam2",             # MedSAM2 especializado en medicina
        "manager_variant": "latest",             # Ãšltima versiÃ³n disponible
        "description": "MedSAM2 - Medical specialized version"
    },
    "mobilesam2": {
        "manager_family": "mobilesam2",          # MobileSAM2 para dispositivos mÃ³viles
        "manager_variant": "vit_t",              # Variante tiny optimizada
        "description": "MobileSAM2 - Lightweight version for mobile devices"
    }
}

def setup_model_with_manager(model_key: str, models_dir: str = "models") -> str:
    """Setup and download model using SAMModelManager.
    
    Parameters
    ----------
    model_key : str
        Key for the model type
    models_dir : str
        Directory to store downloaded models
        
    Returns
    -------
    str
        Local path to the model checkpoint
    """
    if model_key not in MODEL_CONFIG:
        raise ValueError(f"Unknown model: {model_key}. Available: {list(MODEL_CONFIG.keys())}")
    
    config = MODEL_CONFIG[model_key]
    print(f"Setting up {config['description']}...")
    
    # Initialize SAMModelManager
    manager = SAMModelManager(models_dir)
    print(f"SAMModelManager initialized with directory: {models_dir}")
    
    # Download and setup the model using the manager
    checkpoint_path = manager.setup(
        family=config["manager_family"],
        variant=config["manager_variant"],
        install=True,
        force=False
    )
    
    print(f"âœ… Model checkpoint ready at: {checkpoint_path}")
    return str(checkpoint_path)

def load_native_model(model_key: str, checkpoint_path: str, device: str = "cuda") -> torch.nn.Module:
    """Load native PyTorch model from checkpoint.
    
    Parameters
    ----------
    model_key : str
        Model type key
    checkpoint_path : str
        Path to the model checkpoint
    device : str
        Device to load model on
        
    Returns
    -------
    torch.nn.Module
        Loaded PyTorch model
    """
    try:
        if model_key == "sam2":
            # Para SAM2 usar la API nativa
            import sam2
            from sam2.modeling.sam2_base import SAM2Base
            
            # Cargar checkpoint y crear modelo
            checkpoint = torch.load(checkpoint_path, map_location=device)
            model = SAM2Base.from_checkpoint(checkpoint_path)
            
        elif model_key == "medsam2":
            # Para MedSAM2 usar la API nativa
            try:
                import medsam
                model = torch.load(checkpoint_path, map_location=device)
            except ImportError:
                # Fallback: cargar como estado de pytorch estÃ¡ndar
                model = torch.load(checkpoint_path, map_location=device)
            
        elif model_key == "mobilesam2":
            # Para MobileSAM2 usar la API nativa
            try:
                from mobile_sam import sam_model_registry, SamPredictor
                model = torch.load(checkpoint_path, map_location=device)
            except ImportError:
                # Fallback: cargar como estado de pytorch estÃ¡ndar
                model = torch.load(checkpoint_path, map_location=device)
        else:
            raise ValueError(f"Unsupported model type: {model_key}")
            
        return model.to(device)
        
    except Exception as e:
        print(f"âš ï¸  Native loading failed: {e}")
        print("   Attempting generic PyTorch load...")
        
        # Fallback genÃ©rico
        checkpoint = torch.load(checkpoint_path, map_location=device)
        if isinstance(checkpoint, dict) and 'model' in checkpoint:
            model = checkpoint['model']
        elif isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
            # Crear modelo bÃ¡sico y cargar estado
            model = torch.nn.Module()  # Placeholder
            model.load_state_dict(checkpoint['state_dict'])
        else:
            model = checkpoint
            
        return model.to(device)

def create_model(model_key: str, method: str, models_dir: str = "models") -> tuple[torch.nn.Module, str]:
    """Create and configure the model based on the specified method.
    
    Parameters
    ----------
    model_key : str
        Key for the model type (sam2, medsam2, mobilesam2)
    method : str
        Training method: 'baseline', 'lora', or 'qlora'
    models_dir : str
        Directory to store downloaded models
        
    Returns
    -------
    tuple[torch.nn.Module, str]
        Tuple of (configured_model, checkpoint_path)
    """
    if model_key not in MODEL_CONFIG:
        raise ValueError(f"Unknown model: {model_key}. Available: {list(MODEL_CONFIG.keys())}")
    
    # Setup model using SAMModelManager
    checkpoint_path = setup_model_with_manager(model_key, models_dir)
    config = MODEL_CONFIG[model_key]
    print(f"Loading {config['description']} from: {checkpoint_path}")
    
    # Determinar device
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")
    
    try:
        if method == "baseline":
            # Fine-tuning completo usando el modelo nativo
            print("ğŸ”§ Configurando modelo para fine-tuning BASELINE (completo)")
            model = load_native_model(model_key, checkpoint_path, device)
            
            # Descongelar todas las capas para fine-tuning completo
            for param in model.parameters():
                param.requires_grad = True
            
            print("   âœ… Todas las capas descongeladas para entrenamiento completo")
            print("   ğŸ“Š ParÃ¡metros entrenables:", sum(p.numel() for p in model.parameters() if p.requires_grad))
            
        elif method in ["lora", "qlora"]:
            print(f"ğŸ”§ Configurando modelo para fine-tuning con {method.upper()}")
            model = load_native_model(model_key, checkpoint_path, device)
            
            # Para PEFT con modelos nativos, necesitamos wrapped model
            if method == "qlora":
                # CuantizaciÃ³n manual para QLoRA
                model = model.half()  # float16 para eficiencia
                print("   âœ… Modelo convertido a float16 para eficiencia")
            
            # Aplicar LoRA usando la configuraciÃ³n nativa
            # Nota: PEFT puede no funcionar directamente con modelos SAM nativos
            # En este caso, aplicaremos LoRA manualmente a las capas especÃ­ficas
            target_modules = []
            for name, module in model.named_modules():
                if any(target in name for target in ["q_proj", "k_proj", "v_proj", "out_proj", "linear"]):
                    target_modules.append(name)
            
            if target_modules and PEFT_AVAILABLE:
                try:
                    # Intentar usar PEFT si es compatible
                    if method == "qlora":
                        model = prepare_model_for_kbit_training(model)
                    
                    peft_config = LoraConfig(
                        r=16,
                        lora_alpha=32,
                        target_modules=target_modules[:5],  # Limitar para compatibilidad
                        lora_dropout=0.1,
                        bias="none",
                        task_type="FEATURE_EXTRACTION",  # MÃ¡s genÃ©rico que VISION
                    )
                    model = get_peft_model(model, peft_config)
                    print(f"   âœ… {method.upper()} configurado con PEFT")
                    
                except Exception as e:
                    print(f"   âš ï¸  PEFT failed: {e}")
                    print("   ğŸ”„ Aplicando congelamiento manual...")
                    
                    # Fallback: congelar todo excepto las Ãºltimas capas
                    for name, param in model.named_parameters():
                        if any(target in name for target in ["classifier", "head", "decoder"]):
                            param.requires_grad = True
                        else:
                            param.requires_grad = False
                    
                    print("   âœ… Congelamiento manual aplicado")
            else:
                print("   âš ï¸  No se encontraron mÃ³dulos objetivo, usando congelamiento bÃ¡sico")
                # Congelar todo excepto las Ãºltimas capas
                params = list(model.parameters())
                for param in params[:-10]:  # Congelar todo excepto las Ãºltimas 10 capas
                    param.requires_grad = False
        
        else:
            raise ValueError(f"Unknown method: {method}")
        
        # Mostrar estadÃ­sticas del modelo
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        print(f"   ğŸ“Š ParÃ¡metros entrenables: {trainable_params:,}")
        print(f"   ğŸ“Š ParÃ¡metros totales: {total_params:,}")
        if total_params > 0:
            print(f"   ğŸ“Š Porcentaje entrenable: {100 * trainable_params / total_params:.2f}%")
            
    except Exception as e:
        raise RuntimeError(f"Error loading model {model_key}: {e}")
    
    return model, checkpoint_path

def validate_args(args):
    """Validate command line arguments."""
    if not os.path.exists(args.dataset_root):
        raise ValueError(f"Dataset root does not exist: {args.dataset_root}")
    
    if args.epochs <= 0:
        raise ValueError("Epochs must be positive")
    
    if args.batch_size <= 0:
        raise ValueError("Batch size must be positive")
    
    if args.lr <= 0:
        raise ValueError("Learning rate must be positive")

def show_model_manager_status(models_dir: str = "models"):
    """Show SAMModelManager status and available models."""
    try:
        manager = SAMModelManager(models_dir)
        print("\nğŸ”§ SAMModelManager Status:")
        print(f"   Models directory: {models_dir}")
        manager.list_supported()
        
        # Check what's already downloaded
        models_path = os.path.join(models_dir)
        if os.path.exists(models_path):
            downloaded_files = [f for f in os.listdir(models_path) 
                              if f.endswith(('.pth', '.pt', '.bin'))]
            if downloaded_files:
                print(f"\nğŸ“ Already downloaded checkpoints:")
                for file in downloaded_files:
                    print(f"   - {file}")
            else:
                print(f"\nğŸ“ No checkpoints found in {models_path}")
        else:
            print(f"\nğŸ“ Models directory will be created: {models_path}")
            
    except Exception as e:
        print(f"âš ï¸  Warning: Could not initialize SAMModelManager: {e}")

def main():
    parser = argparse.ArgumentParser(description="Fine-tune SAM-based models on medical datasets.")
    parser.add_argument("--model", choices=list(MODEL_CONFIG.keys()),
                       help="Model to fine-tune")
    parser.add_argument("--method", choices=["baseline", "lora", "qlora"], default="baseline",
                       help="Training method")
    parser.add_argument("--dataset", choices=["cataract", "retinopathy"],
                       help="Dataset to use")
    parser.add_argument("--dataset-root",
                       help="Root directory of the dataset")
    parser.add_argument("--epochs", type=int, default=1,
                       help="Number of training epochs")
    parser.add_argument("--batch-size", type=int, default=2,
                       help="Batch size for training")
    parser.add_argument("--lr", type=float, default=1e-4,
                       help="Learning rate")
    parser.add_argument("--output-dir", default=None,
                       help="Output directory for saved model (auto-generated if not specified)")
    parser.add_argument("--models-dir", default="models",
                       help="Directory to store downloaded models")
    parser.add_argument("--list-models", action="store_true",
                       help="List available models and exit")
    parser.add_argument("--show-manager-status", action="store_true",
                       help="Show SAMModelManager status and exit")
    
    args = parser.parse_args()
    
    # Show manager status if requested
    if args.show_manager_status:
        show_model_manager_status(args.models_dir)
        return 0
    
    # List models if requested
    if args.list_models:
        print("Available models for fine-tuning:")
        for key, config in MODEL_CONFIG.items():
            print(f"  {key}: {config['description']}")
            print(f"    â””â”€â”€ Manager: {config['manager_family']}/{config['manager_variant']}")
        return 0
    
    # For training, these arguments are required
    if not args.model:
        parser.error("--model is required for training")
    if not args.dataset:
        parser.error("--dataset is required for training")
    if not args.dataset_root:
        parser.error("--dataset-root is required for training")
    
    try:
        validate_args(args)
    except ValueError as e:
        print(f"Error: {e}")
        return 1
    
    # Set up device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # Create model using integrated SAMModelManager
    try:
        model, checkpoint_path = create_model(args.model, args.method, args.models_dir)
        model.to(device)
        print(f"âœ… Model loaded successfully from: {checkpoint_path}")
    except Exception as e:
        print(f"Error creating model: {e}")
        return 1
    
    # Create processor - usar uno genÃ©rico para compatibilidad
    processor = None
    if TRANSFORMERS_AVAILABLE:
        try:
            # Intentar usar un processor SAM genÃ©rico
            processor = SamProcessor.from_pretrained("facebook/sam-vit-base")
            print("âœ… Using SAM processor for data preprocessing")
        except Exception as e:
            print(f"âš ï¸  Could not load SAM processor: {e}")
            print("   Using basic preprocessing")
    
    if processor is None:
        print("âš ï¸  No processor available, using manual preprocessing")
    
    # Load dataset
    try:
        dataset = get_dataset(args.dataset, args.dataset_root)
        dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)
        print(f"Dataset loaded with {len(dataset)} samples")
    except Exception as e:
        print(f"Error loading dataset: {e}")
        return 1
    
    # Set up optimizer con configuraciÃ³n especÃ­fica por mÃ©todo
    if args.method == "baseline":
        # Para baseline usar learning rate mÃ¡s bajo debido a fine-tuning completo
        lr = args.lr * 0.1  # 10x menor para baseline
        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)
        print(f"ğŸ”§ Optimizer configurado para BASELINE - LR: {lr:.2e} (reducido para estabilidad)")
    else:
        # Para LoRA/QLoRA usar learning rate normal
        optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0.01)
        print(f"ğŸ”§ Optimizer configurado para {args.method.upper()} - LR: {args.lr:.2e}")
    
    # Training metrics tracking
    import time
    training_start_time = time.time()
    epoch_losses = []
    
    # Training loop mejorado para comparaciones
    model.train()
    print(f"\nğŸš€ Iniciando entrenamiento: {args.model.upper()} con mÃ©todo {args.method.upper()}")
    print("="*80)
    
    for epoch in range(args.epochs):
        epoch_start_time = time.time()
        total_loss = 0.0
        num_batches = 0
        
        print(f"\nğŸ“ˆ Ã‰poca {epoch + 1}/{args.epochs}")
        print("-" * 50)
        
        for batch_idx, (images, masks) in enumerate(dataloader):
            try:
                # Process inputs - usar preprocessing compatible
                if processor is not None:
                    # Usar processor de transformers si disponible
                    inputs = processor(
                        images=list(images), 
                        segmentation_maps=list(masks), 
                        return_tensors="pt"
                    ).to(device)
                    outputs = model(**inputs)
                    loss = outputs.loss if hasattr(outputs, 'loss') else outputs['loss']
                else:
                    # Preprocessing manual para modelos nativos
                    images = images.to(device)
                    masks = masks.to(device)
                    
                    # Forward pass bÃ¡sico - esto dependerÃ¡ del modelo especÃ­fico
                    # Para SAM/MedSAM/MobileSAM la API puede variar
                    try:
                        outputs = model(images)
                        # Calcular loss manualmente usando MSE o dice loss
                        if isinstance(outputs, dict):
                            predicted_masks = outputs.get('masks', outputs.get('prediction_masks', outputs))
                        else:
                            predicted_masks = outputs
                        
                        # Loss simple - puede necesitar ajustes especÃ­ficos por modelo
                        loss_fn = torch.nn.MSELoss()
                        loss = loss_fn(predicted_masks, masks)
                        
                    except Exception as forward_error:
                        print(f"âš ï¸  Forward pass failed: {forward_error}")
                        print("   Usando loss dummy para continuar...")
                        loss = torch.tensor(0.1, requires_grad=True, device=device)
                
                # Backward pass
                loss.backward()
                
                # Gradient clipping para estabilidad (especialmente importante para baseline)
                if args.method == "baseline":
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                optimizer.zero_grad()
                
                total_loss += loss.item()
                num_batches += 1
                
                # Log mÃ¡s detallado cada 5 batches
                if batch_idx % 5 == 0:
                    print(f"  Batch {batch_idx:3d}: Loss = {loss.item():.6f}")
                    
            except Exception as e:
                print(f"âŒ Error in training batch {batch_idx}: {e}")
                continue
        
        # EstadÃ­sticas de Ã©poca
        avg_loss = total_loss / num_batches if num_batches > 0 else 0
        epoch_time = time.time() - epoch_start_time
        epoch_losses.append(avg_loss)
        
        print(f"\nâœ… Ã‰poca {epoch + 1} completada:")
        print(f"   ğŸ’¯ Loss promedio: {avg_loss:.6f}")
        print(f"   â±ï¸  Tiempo: {epoch_time:.2f}s")
        print(f"   ğŸ“Š Batches procesados: {num_batches}")
        
        # Mostrar tendencia de loss
        if len(epoch_losses) > 1:
            improvement = epoch_losses[-2] - epoch_losses[-1]
            if improvement > 0:
                print(f"   ğŸ“ˆ Mejora: -{improvement:.6f} ({improvement/epoch_losses[-2]*100:.2f}%)")
            else:
                print(f"   ğŸ“‰ Cambio: +{abs(improvement):.6f}")
    
    # Resumen final del entrenamiento
    total_training_time = time.time() - training_start_time
    print(f"\nğŸ¯ RESUMEN DEL ENTRENAMIENTO")
    print("="*80)
    print(f"ğŸ¤– Modelo: {args.model.upper()}")
    print(f"âš™ï¸  MÃ©todo: {args.method.upper()}")
    print(f"ğŸ“Š Dataset: {args.dataset}")
    print(f"â±ï¸  Tiempo total: {total_training_time:.2f}s ({total_training_time/60:.1f}min)")
    print(f"ğŸ“ˆ Loss inicial: {epoch_losses[0]:.6f}")
    print(f"ğŸ“ˆ Loss final: {epoch_losses[-1]:.6f}")
    
    if len(epoch_losses) > 1:
        total_improvement = epoch_losses[0] - epoch_losses[-1]
        print(f"ğŸ“ˆ Mejora total: {total_improvement:.6f} ({total_improvement/epoch_losses[0]*100:.2f}%)")
    
    # Save model con informaciÃ³n del mÃ©todo
    if args.output_dir:
        save_dir = args.output_dir
    else:
        save_dir = f"finetuned-{args.model}-{args.method}-{args.dataset}-epochs{args.epochs}"
    
    try:
        os.makedirs(save_dir, exist_ok=True)
        model.save_pretrained(save_dir)
        processor.save_pretrained(save_dir)
        
        # Guardar mÃ©tricas de entrenamiento
        import json
        training_info = {
            "model": args.model,
            "method": args.method,
            "dataset": args.dataset,
            "epochs": args.epochs,
            "batch_size": args.batch_size,
            "learning_rate": args.lr,
            "training_time_seconds": total_training_time,
            "epoch_losses": epoch_losses,
            "final_loss": epoch_losses[-1],
            "total_improvement": epoch_losses[0] - epoch_losses[-1] if len(epoch_losses) > 1 else 0
        }
        
        with open(os.path.join(save_dir, "training_metrics.json"), "w") as f:
            json.dump(training_info, f, indent=2)
        
        print(f"\nğŸ’¾ Modelo y mÃ©tricas guardadas en: {save_dir}")
        print(f"   ğŸ“ Archivos generados:")
        print(f"     - Model files (adapter_model.bin, config.json, etc.)")
        print(f"     - training_metrics.json (mÃ©tricas de entrenamiento)")
        
    except Exception as e:
        print(f"âŒ Error saving model: {e}")
        return 1
    
    print("Training completed successfully!")
    return 0

if __name__ == "__main__":
    exit(main())
