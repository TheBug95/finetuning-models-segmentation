# Fine-tuning de Modelos SAM para SegmentaciÃ³n MÃ©dica

Sistema modular y eficiente para fine-tuning de modelos SAM (Segment Anything Model) especializados en segmentaciÃ³n mÃ©dica usando **Hugging Face Transformers**.

## ğŸš€ CaracterÃ­sticas Principales

- **âœ… Modular y POO**: Arquitectura completamente orientada a objetos
- **âœ… Hugging Face First**: Usa modelos oficiales de HF cuando estÃ¡n disponibles
- **âœ… MÃ©todos MÃºltiples**: Soporte para Baseline, LoRA y QLoRA
- **âœ… Datasets Flexibles**: Soporte COCO y formatos estÃ¡ndar
- **âœ… Benchmark Automatizado**: ComparaciÃ³n sistemÃ¡tica de modelos y mÃ©todos

## ğŸ¤– Modelos Soportados

### SAM2 (facebook/sam2-*)
- **tiny**: `facebook/sam2-hiera-tiny` (mÃ¡s rÃ¡pido)
- **base**: `facebook/sam2-hiera-base-plus` 
- **large**: `facebook/sam2-hiera-large`
- **huge**: `facebook/sam2.1-hiera-large` (mejor calidad)

### MedSAM2 (wanglab/*)
- **default**: `wanglab/MedSAM2` (especializado medicina)
- **vit_base**: `wanglab/medsam-vit-base`
- **medsam_mix**: `guinansu/MedSAMix`

### MobileSAM (nielsr/*, qualcomm/*)
- **default**: `nielsr/mobilesam` (optimizado mÃ³vil)
- **qualcomm**: `qualcomm/MobileSam`
- **v2**: `RogerQi/MobileSAMV2`

## âš™ï¸ MÃ©todos de Fine-tuning

| MÃ©todo | DescripciÃ³n | ParÃ¡metros Entrenables | Memoria GPU | Velocidad |
|--------|-------------|----------------------|-------------|-----------|
| **Baseline** | Fine-tuning completo | 100% | Alta | Lento |
| **LoRA** | Low-Rank Adaptation | ~1-5% | Media | Medio |
| **QLoRA** | LoRA + CuantizaciÃ³n 4-bit | ~1-5% | Baja | RÃ¡pido |

## ğŸ“Š Datasets Soportados

- **Cataract**: SegmentaciÃ³n de cataratas (COCO/estÃ¡ndar)
- **Retinopathy**: RetinopatÃ­a diabÃ©tica (COCO/estÃ¡ndar)

## ğŸ”§ InstalaciÃ³n

```bash
# Clonar repositorio
git clone <repo_url>
cd finetuning-models-segmentation

# Instalar dependencias
pip install -r requirements.txt
```

## ğŸš€ Uso RÃ¡pido

### Entrenamiento Individual

```bash
# Entrenar SAM2 con LoRA en dataset de cataratas
python train.py \
    --model sam2 \
    --variant tiny \
    --method lora \
    --dataset cataract \
    --dataset-root ./data/Cataract\ COCO\ Segmentation/train \
    --epochs 5 \
    --batch-size 4

# Entrenar MedSAM2 con QLoRA en retinopatÃ­a
python train.py \
    --model medsam2 \
    --method qlora \
    --dataset retinopathy \
    --dataset-root ./data/Diabetic-Retinopathy\ COCO\ Segmentation/train \
    --epochs 10 \
    --learning-rate 2e-4
```

### Benchmark Completo

```bash
# Comparar todos los modelos y mÃ©todos
python benchmark.py \
    --dataset-root ./data \
    --epochs 3 \
    --output-dir benchmark_results

# Comparar solo LoRA vs QLoRA
python benchmark.py \
    --dataset-root ./data \
    --methods lora qlora \
    --epochs 5
```

## ğŸ“ Estructura del Proyecto

```
finetuning-models-segmentation/
â”œâ”€â”€ models/                     # Modelos modulares
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_model.py          # Clase base
â”‚   â”œâ”€â”€ sam2_model.py          # SAM2 implementation
â”‚   â”œâ”€â”€ medsam2_model.py       # MedSAM2 implementation
â”‚   â””â”€â”€ mobilesam_model.py     # MobileSAM implementation
â”œâ”€â”€ trainers/                   # Entrenadores modulares
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_trainer.py        # Trainer base
â”‚   â”œâ”€â”€ baseline_trainer.py    # Baseline training
â”‚   â”œâ”€â”€ lora_trainer.py        # LoRA training
â”‚   â””â”€â”€ qlora_trainer.py       # QLoRA training
â”œâ”€â”€ datasets/                   # Datasets modulares
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_dataset.py        # Dataset base
â”‚   â”œâ”€â”€ cataract_dataset.py    # Cataract dataset
â”‚   â”œâ”€â”€ retinopathy_dataset.py # Retinopathy dataset
â”‚   â””â”€â”€ dataset_factory.py     # Factory pattern
â”œâ”€â”€ train.py                    # Script principal
â”œâ”€â”€ benchmark.py               # Benchmark automatizado
â”œâ”€â”€ requirements.txt           # Dependencias
â””â”€â”€ README.md                  # Este archivo
```

## ğŸ¯ Arquitectura Modular

### Modelos
```python
from models import SAM2Model, MedSAM2Model, MobileSAMModel

# Cargar modelo
model = SAM2Model(variant="tiny")
model.load_model()
model.load_processor()

# Configurar para fine-tuning
model.setup_for_finetuning("lora", r=16, lora_alpha=32)
```

### Entrenadores
```python
from trainers import LoRATrainer

# Crear entrenador
trainer = LoRATrainer(model, learning_rate=1e-4)
trainer.setup_model_for_training()
trainer.setup_optimizer()

# Entrenar
metrics = trainer.train(dataloader, epochs=5)
```

### Datasets
```python
from datasets import create_dataset

# Crear dataset
dataset = create_dataset("cataract", "./data/cataract", split="train")
dataloader = DataLoader(dataset, batch_size=4)
```

## ğŸ“Š Resultados de Benchmark

El script `benchmark.py` genera automÃ¡ticamente:

- **CSV Report**: `benchmark_report.csv` con mÃ©tricas detalladas
- **JSON Stats**: `benchmark_stats.json` con estadÃ­sticas agregadas
- **Raw Results**: `benchmark_results.json` con resultados completos

### MÃ©tricas Incluidas
- Tiempo de entrenamiento
- Loss final y mejor loss
- ParÃ¡metros entrenables vs totales
- Tasa de Ã©xito por modelo/mÃ©todo
- Uso de memoria (estimado)

## ğŸ” Comandos Ãštiles

```bash
# Listar modelos disponibles
python train.py --list-models

# Listar datasets disponibles
python train.py --list-datasets

# Entrenar con configuraciÃ³n personalizada de LoRA
python train.py \
    --model sam2 \
    --method lora \
    --dataset cataract \
    --dataset-root ./data/cataract \
    --lora-r 32 \
    --lora-alpha 64 \
    --lora-dropout 0.05

# Benchmark solo modelos especÃ­ficos
python benchmark.py \
    --dataset-root ./data \
    --models sam2 medsam2 \
    --methods lora qlora
```

## ğŸ¨ Extensibilidad

### AÃ±adir Nuevo Modelo
```python
from models.base_model import BaseSegmentationModel

class MyNewModel(BaseSegmentationModel):
    def load_model(self):
        # Implementar carga del modelo
        pass
    
    def load_processor(self):
        # Implementar carga del procesador
        pass
```

### AÃ±adir Nuevo Dataset
```python
from datasets.base_dataset import BaseMedicalDataset

class MyNewDataset(BaseMedicalDataset):
    def _load_data_list(self):
        # Implementar carga de datos
        pass
```

## ğŸš¨ Mejores PrÃ¡cticas

1. **Usar HF Models**: Siempre preferir modelos de Hugging Face
2. **Modularidad**: Mantener componentes separados y reutilizables
3. **Error Handling**: Manejo robusto de errores en cada componente
4. **Logging**: Logging detallado para debugging
5. **Testing**: Probar componentes individualmente antes de integrar

## ğŸ”§ ConfiguraciÃ³n de GPU

```bash
# Verificar GPU disponible
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"

# Para mÃºltiples GPUs (futuro)
export CUDA_VISIBLE_DEVICES=0,1
```

## ğŸ“ˆ Roadmap

- [ ] Soporte para mÃºltiples GPUs
- [ ] IntegraciÃ³n con Weights & Biases
- [ ] MÃ¡s mÃ©tricas de evaluaciÃ³n (IoU, Dice, etc.)
- [ ] Soporte para mÃ¡s datasets mÃ©dicos
- [ ] OptimizaciÃ³n automÃ¡tica de hiperparÃ¡metros
- [ ] ExportaciÃ³n a formatos de inferencia (ONNX, TensorRT)

## ğŸ¤ Contribuciones

1. Fork el repositorio
2. Crear branch para nueva feature (`git checkout -b feature/nueva-feature`)
3. Commit cambios (`git commit -am 'AÃ±adir nueva feature'`)
4. Push al branch (`git push origin feature/nueva-feature`)
5. Crear Pull Request

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

Los datasets pueden estar organizados de dos maneras:

1. **Directorios separados** de imÃ¡genes y mÃ¡scaras binarias:

```text
<root>/
    images/
        xxx.png
    masks/
        xxx.png
```

2. **Formato COCO** (por ejemplo exportado desde Roboflow), donde cada divisiÃ³n
contiene todas las imÃ¡genes y un archivo `_annotations.coco.json`:

```text
<root>/
    train/
        _annotations.coco.json
        img1.png
        ...
    valid/
        _annotations.coco.json
        ...
```

En este caso, proporcione la ruta del subconjunto deseado (`train`, `valid`,
etc.) mediante el parÃ¡metro `--dataset-root`.

### Ejemplo de EjecuciÃ³n

```bash
# Fine-tuning baseline (para establecer lÃ­nea base de comparaciÃ³n)
python finetune.py --model sam2 --method baseline --dataset cataract --dataset-root /ruta/al/dataset --epochs 5

# Fine-tuning con LoRA (recomendado para la mayorÃ­a de casos)
python finetune.py --model medsam2 --method lora --dataset retinopathy --dataset-root /ruta/al/dataset --epochs 5 --batch-size 4

# Fine-tuning con QLoRA (para GPUs con memoria limitada)
python finetune.py --model mobilesam2 --method qlora --dataset cataract --dataset-root /ruta/al/dataset --lr 5e-5

# ComparaciÃ³n automÃ¡tica de todos los modelos y mÃ©todos
python compare_models.py --dataset-root /ruta/al/dataset --epochs 3

# Listar modelos disponibles
python finetune.py --list-models

# Ver estado del gestor de modelos
python finetune.py --show-manager-status
```

### ParÃ¡metros Disponibles

- `--model`: Modelo a entrenar (`sam2`, `medsam2`, `mobilesam2`)
- `--method`: MÃ©todo de entrenamiento (`baseline`, `lora`, `qlora`)
- `--dataset`: Dataset a usar (`cataract`, `retinopathy`)
- `--dataset-root`: Directorio raÃ­z del dataset
- `--epochs`: NÃºmero de Ã©pocas (default: 1)
- `--batch-size`: TamaÃ±o del batch (default: 2)
- `--lr`: Learning rate (default: 1e-4)
- `--output-dir`: Directorio de salida (se genera automÃ¡ticamente si no se especifica)
- `--models-dir`: Directorio para modelos descargados (default: "models")
- `--list-models`: Listar modelos disponibles
- `--show-manager-status`: Mostrar estado del SAMModelManager

El parÃ¡metro `--method` acepta:

- `baseline`: Entrenamiento completo sin optimizaciones
- `lora`: Low-Rank Adaptation para fine-tuning eficiente
- `qlora`: LoRA cuantizado para GPUs con memoria limitada

Los modelos y el procesador resultante se guardan en un directorio con el
formato `finetuned-<modelo>-<metodo>-<dataset>-epochs<num_epochs>`.

## ğŸ”¬ ComparaciÃ³n de MÃ©todos

### Script de ComparaciÃ³n AutomÃ¡tica

El proyecto incluye `compare_models.py` para ejecutar comparaciones sistemÃ¡ticas:

```bash
# Comparar todos los modelos con todos los mÃ©todos
python compare_models.py --dataset-root /ruta/al/dataset

# Comparar solo mÃ©todos especÃ­ficos
python compare_models.py --dataset-root /ruta/al/dataset --methods baseline lora

# Comparar solo modelos especÃ­ficos
python compare_models.py --dataset-root /ruta/al/dataset --models sam2 medsam2

# Vista previa de experimentos sin ejecutar
python compare_models.py --dataset-root /ruta/al/dataset --dry-run
```

### MÃ©tricas de ComparaciÃ³n

Cada experimento genera automÃ¡ticamente:

- **training_metrics.json**: MÃ©tricas detalladas del entrenamiento
- **comparison_report.md**: Reporte comparativo en Markdown
- **Tiempo de entrenamiento**: Para analizar eficiencia
- **PÃ©rdida final**: Para comparar convergencia
- **Uso de memoria**: Para optimizar recursos

### Resultados Esperados

| MÃ©todo | Memoria GPU | Tiempo | Calidad | Recomendado para |
|--------|-------------|--------|---------|------------------|
| **Baseline** | Alta | Alto | MÃ¡xima | ComparaciÃ³n y mÃ¡ximo rendimiento |
| **LoRA** | Media | Medio | Alta | Uso general y producciÃ³n |
| **QLoRA** | Baja | Medio | Buena | GPUs con memoria limitada |

## GestiÃ³n de Modelos

Para descargar e instalar los pesos oficiales de las distintas variantes de
SAM, MobileSAM, HQ-SAM y MedSAM(+2) se incluye el mÃ³dulo
`model_manager.py`. Este expone la clase `SAMModelManager`, que automatiza la
instalaciÃ³n de los repositorios y la obtenciÃ³n de los checkpoints.

### Ejemplo de Uso del Model Manager

```python
from model_manager import SAMModelManager

# Crear el gestor
mgr = SAMModelManager("Models")

# Listar modelos soportados
mgr.list_supported()

# Descargar e instalar un modelo especÃ­fico
ckpt_path = mgr.setup("mobilesam", "vit_t")

# Solo descargar sin instalar dependencias
ckpt_path = mgr.download_variant("sam", "vit_b")
```

Los archivos descargados se guardarÃ¡n dentro del directorio indicado (en el
ejemplo, `Models`).

### Modelos Disponibles en Model Manager

- **sam**: ViT-B, ViT-L, ViT-H
- **mobilesam**: ViT-T
- **hq_sam**: ViT-B, ViT-L, ViT-H, ViT-T, Hiera-L-2.1
- **medsam**: MedSAM1, MedSAM2-latest

## Mejoras Implementadas

- âœ… **IntegraciÃ³n completa SAMModelManager**: Setup automÃ¡tico de modelos
- âœ… **Manejo robusto de errores**: ValidaciÃ³n en todas las funciones crÃ­ticas
- âœ… **Logging detallado**: InformaciÃ³n completa durante el entrenamiento
- âœ… **VerificaciÃ³n automÃ¡tica**: DetecciÃ³n de estructura de datasets
- âœ… **EliminaciÃ³n de cÃ³digo duplicado**: Arquitectura modular y limpia
- âœ… **DocumentaciÃ³n completa**: GuÃ­as detalladas y ejemplos
- âœ… **Dependencias especÃ­ficas**: Versiones compatibles y tested
- âœ… **Cache inteligente**: Los modelos se descargan una sola vez
- âœ… **Fallbacks automÃ¡ticos**: Robustez ante fallos de descarga

## ğŸ“‹ Requisitos

Ver `requirements.txt` para la lista completa de dependencias.

## ğŸ¯ Mejores PrÃ¡cticas

### ConfiguraciÃ³n de GPU

- **Memoria recomendada**: 12GB+ para baseline, 8GB+ para LoRA, 6GB+ para QLoRA
- **Uso de mixed precision**: Habilitado automÃ¡ticamente para optimizar memoria
- **Gradient checkpointing**: Activado para reducir uso de memoria

### HiperparÃ¡metros

- **Learning rate baseline**: 1e-5 (mÃ¡s bajo que LoRA/QLoRA)
- **Gradient clipping**: 1.0 para estabilizar entrenamiento baseline
- **Batch size**: Ajustar segÃºn memoria disponible
- **Ã‰pocas**: 3-5 para datasets pequeÃ±os, 10+ para datasets grandes

### Monitoreo del Entrenamiento

- Usar `wandb` o `tensorboard` para seguimiento de mÃ©tricas
- Revisar `training_metrics.json` para anÃ¡lisis detallado
- Comparar curvas de pÃ©rdida entre mÃ©todos

### SelecciÃ³n de MÃ©todo

1. **Baseline**: Para obtener el mÃ¡ximo rendimiento posible y establecer una lÃ­nea base
2. **LoRA**: Para la mayorÃ­a de casos de uso prÃ¡cticos con buen balance rendimiento/eficiencia
3. **QLoRA**: Para GPUs con memoria limitada manteniendo buena calidad

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el repositorio
2. Crea una rama para tu feature
3. Documenta los cambios
4. EnvÃ­a un pull request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Ver el archivo LICENSE para mÃ¡s detalles.
